{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"teaser\" style=' background-position: right center; background-size: 00px; background-repeat: no-repeat; \n",
    "    padding-top: 30px;\n",
    "    padding-right: 10px;\n",
    "    padding-bottom: 110px;\n",
    "    padding-left: 10px;\n",
    "    background-color: #FFFEE9;\n",
    "    border-radius: 15px;                    \n",
    "    border-bottom: 4px solid #379;                    \n",
    "    border-top: 4px solid #379;\n",
    "    border-right: 4px solid #379;\n",
    "    border-left: 4px solid #379;' > \n",
    "\n",
    "   <div style=\"text-align:center\">\n",
    "    <b><font size=\"6.7\">Analysing dynamics data with unsupervised learning</font></b><br><br><br> \n",
    "    <b><font size=\"4.5\">Machine Learning and Quantum Computing for Quantum Molecular Dynamics - 2022</font></b><br><br> \n",
    "  </div>\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    " <font size=\"3.0\"> Author: Max Pinheiro Jr<sup>1</sup></font><br><br>\n",
    " <font size=\"3.0\"> <sup>1</sup>Aix Marseille University, CNRS, Marseille, France<br>\n",
    "<span class=\"cecam--last-updated\" data-version=\"v1.0.0\">[Last updated: August 24, 2022]</span>\n",
    "</p>   \n",
    "  \n",
    "<div> \n",
    "<img  style=\"padding-top: 15px; padding-left: 30px; padding-right: 30px; float: left;\" \n",
    "     src=\"https://www.cinam.univ-mrs.fr/cinam/wp-content/uploads/2018/11/logo-amu.png\" width=\"230\">    \n",
    "<img  style=\"padding-top: 20px; padding-left: 30px; padding-right: 30px; float: right;\" \n",
    "     src=\"https://www.cecam.org/themes/cecam/assets/images/logo.png\" width=\"250\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is devoted to the application of unsupervised machine learning methods to analyze and interpret chemical data generated by molecular dynamics simulations. Broadly speaking, the main goal in unsupervised learning is to find natural grouping structure or possible associations within the data, or even a compact representation for the data based on measures on a set of inputs. Many algorithms have been developed to accomplish these tasks being *dimensionality reduction* and *clustering analysis* the two most representative and important branches of unsupervised learning. Here we will explore some of the these techniques for data analysis by considering practical examples based on a molecular configurational dataset generated from snapshots of molecular dynamics simulations to facilitate your understanding of the theory underpinning the methods. Instead of going deep into mathematical details we will dive into a few introductory examples of the methods (when viable) using simplified or *toy* data to explain and illustrate how the algorithms work in practice. <br><br> The tutorial was designed to run in a Python environment, so a basic knowledge of this programming language will be helpful. For those that are not familiar with Python, I suggest you to take a time to check the [Python documentation](https://docs.python.org/3/) and the [Numpy manual](https://numpy.org/doc/1.18/) for clues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Links to access the tutorial:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/maxjr82/CECAM-MLQCDyn/blob/main/tutorial/UL_for_MD.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/maxjr82/CECAM-MLQCDyn/blob/main/tutorial/UL_for_MD.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "\n",
    "- To run the code cells in the *google colab* platform, it will be necessary to install a few non-standard Python packages. This can be done by copying the linux command shown below and running it in a normal code cell of the google colab environment. Note that to run any Linux command the code line must start with \"!\".<br><br>**! pip install py3Dmol**<br><br>\n",
    "\n",
    "- To run the tutorial exercises locally, you will need first to download the corresponding molecular dataset from the MD17 database. The Linux command to download the dataset is provided below and it should be executed in a code cell of Jupyter notebook or in google colab:<br><br> **! wget http://quantum-machine.org/gdml/data/npz/ + molecule_name + _dft.npz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-16T09:41:59.912000Z",
     "start_time": "2021-01-16T09:41:59.909266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use this code cell to install the packages you need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "The topics covered by this tutorial are organized as follows:\n",
    " \n",
    "1. [Dimensionality reduction](#dim_reduction)\n",
    "    1. [Principal Component Analysis (PCA): a brief introduction üè°](#pca_intro)\n",
    "    2. [Tutorial I: applying PCA to a molecular dynamics dataset üñ•Ô∏è](#tutor1_pca)\n",
    "    3. [Nonlinear dimensionality reduction: kernelizing PCA üè°](#kpca_intro)\n",
    "    3. [Tutorial II: nonlinear compression of MD data with kernel PCA üñ•Ô∏è](#tutor2_kpca)\n",
    "3. [Clustering analysis](#clustering)\n",
    "    1. [K-Means: a brief introduction üè°](#kmeans)\n",
    "    2. [Tutorial III: clustering MD data with K-Means üñ•Ô∏è](#tutor3_kmeans)\n",
    "    3. [Evaluation metrics for clustering](#optimal_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dim_reduction'></a>\n",
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of dimensionality reduction as an unsupervised learning technique is to scale down the original dimension of the data by removing redundant or noisy features in order to obtain a compact representation of the data that still preserve its structure and usefulness. In this way, dimensionality reduction algorithms can be used either as a data exploration tool that enables a visual inspection of the data structure via scatter plots or as a preprocessing step to provide effectively compact input data for supervised learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca_intro'></a>\n",
    "## Principal Component Analysis (PCA): a brief introduction üè°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)** is a linear dimension reduction algorithm that works by finding a **mutually orthogonal** set of axes (i.e., linearly independent) that accounts for the largest variance in the data. These set of linearly uncorrelated axes are called *principal components* (PCs) and can equivalently be defined as the directions that **maximizes the variance of the projected data**. Thus, PCA is a technique from linear algebra that aims to represent the data within the lowest possible dimension (that is, using the fewest number of principal components) while still preserving the most relevant information contained in the original data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T10:46:09.966884Z",
     "start_time": "2022-07-15T10:46:09.879828Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "vid = YouTubeVideo('nEvKduLXFvk', width=600, height=450)\n",
    "display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the PCs can be obtained in a closed-form solution either by solving the **eigenvalue problem of the covariance matrix** or via **singular value decomposition**. One can also find the optimal reduced subspace with PCA by computing the so-called **reconstruction error** which measure how much the data projected onto the lower dimensional space spanned by the $k$-PCs deviates or differ from the original input data as given by the Euclidean distance between the original and reconstructed data points. This difference comes from the fact that compressing the data to a lower dimension always comes at the expense of some information loss. However, by [**minimizing the reconstruction error**](https://people.eecs.berkeley.edu/~jordan/courses/294-fall09/lectures/dimensionality/paper-1x2.pdf), we enforce that such an information loss be as small as possible. It can be shown that the reconstruction error corresponds to the sum of the eigenvalues of the covariance matrix for the $m$ discarded principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General applications:**\n",
    "\n",
    "  - visualization of high dimensional data \n",
    "  - reduce computational cost for ML\n",
    "  - feature extraction \n",
    "  - noise-filtration \n",
    "  \n",
    "**In computational chemistry:**\n",
    "\n",
    "- [characterization of chemical reaction pathways from molecular dynamics simulations](https://pubs.rsc.org/en/content/articlelanding/2019/SC/C9SC02742D#!divAbstract)\n",
    "- [improve the quality of molecular descriptor for neural network potentials](https://aip.scitation.org/doi/10.1063/5.0009264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:47:40.792508Z",
     "start_time": "2022-08-25T12:47:39.429415Z"
    }
   },
   "outputs": [],
   "source": [
    "# Math libraries\n",
    "import numpy as np\n",
    "\n",
    "# Data manipulation \n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA algorithm from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T05:50:35.676672Z",
     "start_time": "2022-08-19T05:50:35.671075Z"
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    '''\n",
    "    Implementation of the PCA method.\n",
    "    \n",
    "    Class attributes:\n",
    "        n_components: the number of leading dimensions after data compression.\n",
    "        \n",
    "    Argument of the functions:\n",
    "        X: A MxN dataset as NumPy array where the samples are stored as rows (M),\n",
    "           and the features defined as columns (N).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        # STEP 1: Center the data by the mean, i.e, make the\n",
    "        # the sample mean of each column equal to zero.\n",
    "        # WARNING: check for outliers!\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X - self.mean\n",
    "        # STEP2: Calculate the covariance matrix of (X - X_mean)\n",
    "        # In numpy, the cov function needs samples as columns\n",
    "        cov = np.cov(X.T)\n",
    "        # STEP 3: Compute eigenvalues and eigenvectors\n",
    "        # np.eigh function returns them in ascending order\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        # STEP 4: sort eigenvectors and transpose them for easier calculations\n",
    "        eigvals, eigvecs = eigvals[::-1], eigvecs[:, ::-1]\n",
    "        eigvecs = eigvecs.T\n",
    "        # STEP 5: store the first n eigenvectors\n",
    "        self.components = eigvecs[0:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X - self.mean\n",
    "        # project the data along the n PCA components by computing the scalar product\n",
    "        X_trans = np.dot(X, self.components.T)\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test the PCA class above on a simple 2D array of data points representing a linear distribution with some noise added to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T05:50:39.105458Z",
     "start_time": "2022-08-19T05:50:38.780321Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate data from a linear equation: y = m*x + c\n",
    "m, c = 0.5, -3\n",
    "X = [[i,m*i+np.random.uniform(10) + c] for i in range(-20,20)]\n",
    "X = np.array(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply our PCA algorithm keeping the 2 components of the feature space. Note that in this case we are not reducing the dimensionality of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T05:50:44.711184Z",
     "start_time": "2022-08-19T05:50:44.706869Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_transformed = pca.transform(X)\n",
    "# Each column of the matrix below correspond to one PC axis\n",
    "print(pca.components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>üìù Important:</b> Note that each of the PCA components must be mutually orthogonal. You can check this property by calculating the scalar product between the columns of the matrix above using, for example, the np.dot function of Numpy. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a linear method, PCA has the important advantage of being easily explainable. Indeed, each principal component has the so-called **explained variance** associated with it, which essentially tells us, in percentage terms, how much of the total variance in the data is captured by each one of the principal components (PC). The variance obtained for each PC is sorted in a decreasing order, so that PC1 is the component contributing the most to the explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:36:43.052846Z",
     "start_time": "2022-07-18T15:36:43.046361Z"
    }
   },
   "outputs": [],
   "source": [
    "def explained_variance(X_data):\n",
    "    '''This function calculates the fraction of variance explained by a principal component for an \n",
    "    input dataset X_data provided as a numpy array. To do that, the variance is calculated for each \n",
    "    column of the data and then divided by the total variance.''' \n",
    "    \n",
    "    n_sample, n_cols = X_data.shape\n",
    "    # compute the variance of the whole data set\n",
    "    total_var = (X_data**2).sum()/(n_sample-1)\n",
    "    # variance of selected columns\n",
    "    vars_col = np.array([np.var(X_data[:,i], ddof=1) for i in range(n_cols)])\n",
    "    explained_var_ratio = vars_col/total_var\n",
    "    \n",
    "    return explained_var_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:36:43.256776Z",
     "start_time": "2022-07-18T15:36:43.246992Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explained_var_original = explained_variance(X)\n",
    "explained_var_transformed = explained_variance(X_transformed)\n",
    "print(\"******************\")\n",
    "print(\"  Variance ratio:\")\n",
    "print(\"******************\")\n",
    "print(\" \")\n",
    "print(\"Original data\")\n",
    "print(\"  X1  {:2.2f} \\n  X2  {:2.2f}\".format(explained_var_original[0]*100,\n",
    "                                             explained_var_original[1]*100))\n",
    "print(\" \")\n",
    "print(\"Transformed data\")\n",
    "print(\"  PC1  {:2.2f} \\n  PC2  {:2.2f}\".format(explained_var_transformed[0]*100,\n",
    "                                               explained_var_transformed[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA algorithm is also available in the **Scikit-Learn** package (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), one of the most popular toolkit used for machine learning and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:37:40.011105Z",
     "start_time": "2022-07-18T15:37:40.006173Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate the PCA object with the desired number\n",
    "# of dimensions to keep (n_components = 1, 2, 3,...)\n",
    "pca_n2 = PCA(n_components=2)\n",
    "pca_n2.fit(X)\n",
    "X_trans_pc2 = pca_n2.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the PCA components and the explained variance ratio calculated using the scikit-learn library with those ones obtained previously with our PCA implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:37:40.961806Z",
     "start_time": "2022-07-18T15:37:40.953821Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"**********************************\")\n",
    "print(\"Principal components from sklearn:\")\n",
    "print(\"**********************************\")\n",
    "print(\" \")\n",
    "print(pca_n2.components_)\n",
    "print(\" \")\n",
    "print(\"**************************************\")\n",
    "print(\"Explained variance ratio from sklearn:\")\n",
    "print(\"**************************************\")\n",
    "print(\" \")\n",
    "print(pca_n2.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purpose, let's also project the data with PCA using only one principal component. In this case, the reduced data can be plotted along a single straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:38:17.464821Z",
     "start_time": "2022-07-18T15:38:17.460199Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_n1 = PCA(n_components=1)\n",
    "pca_n1.fit(X)\n",
    "X_trans_pc1 = pca_n1.transform(X)\n",
    "X_reconstructed = pca_n1.inverse_transform(X_trans_pc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:38:17.851661Z",
     "start_time": "2022-07-18T15:38:17.845369Z"
    }
   },
   "outputs": [],
   "source": [
    "X_trans_pc1 = np.concatenate([X_trans_pc1, np.zeros_like(X_trans_pc1)], axis=1)\n",
    "X_trans_pc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-18T15:46:06.230555Z",
     "start_time": "2022-07-18T15:46:05.932636Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color=\"black\")\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "fig, (ax1, ax2) =  plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "\n",
    "# plot the original data on ax1\n",
    "ax1.scatter(X[:, 0], X[:, 1], alpha=0.3, c=\"blue\", label = \"original\")\n",
    "ax1.scatter(X_reconstructed[:,0], X_reconstructed[:,1], alpha=0.5, \n",
    "            marker = \"D\", c=\"red\", label = \"reduced\")\n",
    "for length, vector in zip(pca_n2.explained_variance_, pca_n2.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca_n2.mean_, pca_n2.mean_ + v, ax1)\n",
    "ax1.set_xlabel(\"X1\")\n",
    "ax1.set_ylabel(\"X2\")\n",
    "ax1.set_title(\"Original data vs PC1\", fontsize=16)\n",
    "ax1.axis('equal')\n",
    "ax1.legend(loc=\"upper left\", shadow=True, fancybox=True, fontsize=14)\n",
    "\n",
    "# plot the projected data on ax2\n",
    "ax2.scatter(X_trans_pc2[:,0], X_trans_pc2[:,1], alpha=0.3,\n",
    "            c=\"blue\", label = \"n_components = 2\")\n",
    "ax2.scatter(X_trans_pc1[:,0], X_trans_pc1[:,1], alpha=0.5,\n",
    "            marker = \"D\", c=\"red\", label = \"n_components = 1\")\n",
    "ax2.set_xlabel(\"Principal Component 1\")\n",
    "ax2.set_ylabel(\"Principal Component 2\")\n",
    "ax2.set_title(\"Transformed data (PC1 vs PC2)\", fontsize=16)\n",
    "ax2.axis('equal')\n",
    "ax2.legend(loc=\"upper right\", shadow=True, fancybox=True, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tutor1_pca'></a>\n",
    "## Tutorial I: applying PCA to a molecular dynamics dataset üñ•Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The MD17 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:47:48.229109Z",
     "start_time": "2022-08-25T12:47:48.181185Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def get_md17_data(molecule_name: str) -> dict:\n",
    "    '''Retrieve the MD17 dataset for a specified molecule directly from the website.'''\n",
    "    md17_data_address = \"http://quantum-machine.org/gdml/data/npz/\"\n",
    "    full_path = md17_data_address + molecule_name.lower() + \"_dft.npz\"\n",
    "    r = requests.get(full_path, stream = True)\n",
    "    data = np.load(BytesIO(r.raw.read()))\n",
    "    return dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be stored in memory as Python dictonary, where the keys store the ab initio molecular dynamics data for different quantities as well as metadata describing the dataset and the molecular system. In our exercise, the two most important quantities will be the *total energy (kcal/mol)* and the *molecular geometries (Angstroms)* which can be accessed with the keys E and R, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:48:06.563953Z",
     "start_time": "2022-08-25T12:47:48.542459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the commands below to read the from a local directory \n",
    "#data = dict(np.load('aspirin_dft.npz'))\n",
    "#data.keys()\n",
    "\n",
    "data = get_md17_data('aspirin')\n",
    "for k in data.keys():\n",
    "    content = data[k].shape\n",
    "    if data[k].size == 1:\n",
    "        content = data[k]\n",
    "    print(\"{} ---> {}\".format(k,content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying any ML model (either supervised or unsupervised), it is always a good and recommended practice to take a quick look at the data. So let's start by visualizing some frames of the aspirin MD data to see what the molecular structures look like. To do that, we will use the **py3Dmol** package that uses the Cartesian coordinates and atom labels to render the molecular structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:52:17.698840Z",
     "start_time": "2022-08-25T12:52:17.692473Z"
    }
   },
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "\n",
    "def create_xyz_str(atom_labels: np.ndarray, xyz_matrix: np.ndarray) -> str:\n",
    "    mol = np.concatenate((atom_labels, xyz_matrix), axis=1)\n",
    "    n_atoms = len(atom_labels)\n",
    "    assert n_atoms == xyz_matrix.shape[0]\n",
    "    xyz_str = [str(i).strip('[]') for i in mol]\n",
    "    geom = str(n_atoms) + '\\n' + ' ' + '\\n'\n",
    "    geom += '\\n'.join(xyz_str)\n",
    "    geom = geom.replace(\"'\", \"\")\n",
    "    geom += '\\n'\n",
    "    return geom\n",
    "    \n",
    "def view_molecule(data: dict, index: [int,list]) -> py3Dmol.view:\n",
    "    \n",
    "    symbols = {1:'H', 6:'C', 7:'N', 8:'O'}\n",
    "    atom_number = data['z']\n",
    "    n_atoms = atom_number.size\n",
    "    labels = np.vectorize(symbols.get)(atom_number)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    \n",
    "    molview = py3Dmol.view(width=380,height=380)\n",
    "    \n",
    "    if isinstance(index, list):\n",
    "        geoms = \"\"\n",
    "        for i in index:\n",
    "            geoms += create_xyz_str(labels, data['R'][i])   \n",
    "        molview.addModelsAsFrames(geoms)\n",
    "        molview.animate({'loop': \"forward\"})\n",
    "    else:\n",
    "        geom = create_xyz_str(labels, data['R'][index])\n",
    "        molview.addModel(geom,'xyz')\n",
    "    \n",
    "    style = {'stick': {'radius': .13}, 'sphere': {'scale': 0.21}}    \n",
    "    molview.setStyle(style)\n",
    "    molview.setBackgroundColor('#FFFEE9')\n",
    "    molview.zoomTo()\n",
    "    \n",
    "    return molview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T12:57:59.148361Z",
     "start_time": "2022-08-25T12:57:58.680330Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geom_indices = list(range(19600,20000))\n",
    "mol = view_molecule(data, geom_indices)\n",
    "mol.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T08:36:39.135041Z",
     "start_time": "2022-08-25T08:36:39.130867Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def inv_dist_descriptor(xyz_matrix: np.ndarray) -> np.ndarray:\n",
    "    '''Calculate the inverse pairwise distance descriptor from Cartesian coordinates.'''\n",
    "    n_samples, n_atoms, _ = xyz_matrix.shape\n",
    "    dist_vec_size = int(n_atoms * (n_atoms - 1)/2)\n",
    "    r2_matrix = np.empty((n_samples, dist_vec_size))\n",
    "    \n",
    "    for idx in range(n_samples):\n",
    "        distance_matrix = np.zeros((n_atoms, n_atoms))\n",
    "        geom = xyz_matrix[idx]\n",
    "        for i, j in combinations(range(len(geom)), 2):\n",
    "            R = np.linalg.norm(geom[i] - geom[j])\n",
    "            distance_matrix[j, i] = R\n",
    "\n",
    "        r2_vector = distance_matrix[np.tril_indices(len(distance_matrix), -1)]\n",
    "        r2_matrix[idx, :] = r2_vector.reshape(1,-1)\n",
    "        \n",
    "    return 1/r2_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T08:40:21.998598Z",
     "start_time": "2022-08-25T08:36:39.136154Z"
    }
   },
   "outputs": [],
   "source": [
    "X = inv_dist_descriptor(data['R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:40:21.280812Z",
     "start_time": "2022-08-25T09:40:21.236055Z"
    }
   },
   "outputs": [],
   "source": [
    "# These preprocessing functions of sklearn can be used for rescaling data\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "idx_samples = np.random.choice(X.shape[0], size=10000, replace=False)\n",
    "X_train = X[idx_samples]\n",
    "X_train = StandardScaler().fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>üìå Info: </b> \n",
    "    Data normalization can be an essential preprocessing step for some machine learning algorithms. Because PCA is essentialy a variance maximizing exercise, the algorithm will give higher weights for selecting features that contribute the most for the covariance matrix. Thus, if the features in the data set are not varying in a comparable scale, the PCA decomposition may lead to unreliable or artifficial results. To know more about the importance of data scaling for PCA and other ML algorithms, check out <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py\">this link</a>. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:40:23.072569Z",
     "start_time": "2022-08-25T09:40:22.822361Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_n2 = PCA(n_components=2)\n",
    "pca_n2.fit(X_train)\n",
    "X_transformed = pca_n2.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:40:23.214164Z",
     "start_time": "2022-08-25T09:40:23.209607Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_n2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To practice üí™:<br><br>In the example above, the first two principal components (PC) obtained after applying the PCA dimension reduction can explain only 24% of the variability in the data. How many PC components one needs to keep in order to explain at least 95% of the data variance with PCA? In addition, make a plot of the cumulative explained variance ratio as a function of the number of principal components for the PCA transformed dataset.**\n",
    "\n",
    "Hint: Check the acceptable values for the *n_components* argument of the PCA method in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:40:25.954411Z",
     "start_time": "2022-08-25T09:40:25.701998Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epot_train = data['E'][idx_samples]\n",
    "epot_min = np.min(data['E'])\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], s=5, alpha=0.8,\n",
    "            c=epot_train-epot_min, cmap='jet')\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "cbar = plt.colorbar(pad=0.02)\n",
    "cbar.set_label(\"Relative energy (kcal/mol)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretating the results:** \n",
    "\n",
    "What do the two isolated clusters mean in terms of molecular conformation? Or we could also ask which (geometric) features of the molecule (e.g., bond lengths, angles or dihedrals) is mainly responsible for splitting the data into two big groups? ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:40:28.591148Z",
     "start_time": "2022-08-25T09:40:28.587961Z"
    }
   },
   "outputs": [],
   "source": [
    "# ASE Documentation: \n",
    "# https://wiki.fysik.dtu.dk/ase/index.html\n",
    "import ase\n",
    "\n",
    "def get_dihedrals(coords: np.ndarray, symbols: list, indices: list) -> np.ndarray:\n",
    "    dihedrals = []\n",
    "    for xyz in coords:\n",
    "        mol = ase.Atoms(symbols=labels, positions=xyz)\n",
    "        dih_angle = mol.get_dihedral(*indices)\n",
    "        if dih_angle > 180:\n",
    "            dih_angle = 360 - dih_angle\n",
    "        dihedrals.append(dih_angle)\n",
    "    dihedrals = np.array(dihedrals)    \n",
    "    return dihedrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:02.469813Z",
     "start_time": "2022-08-25T09:40:30.433785Z"
    }
   },
   "outputs": [],
   "source": [
    "# These atoms correspond to the carboxylic group, COOH\n",
    "atom_indices = [0,5,10,9]\n",
    "labels = ['C', 'C', 'C', 'C', 'C', 'C', 'C', 'O', 'O', 'O', 'C', \n",
    "          'C', 'O', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H']\n",
    "dih = get_dihedrals(data['R'], labels, atom_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:03.245839Z",
     "start_time": "2022-08-25T09:41:02.471323Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.displot(dih, kde=True, aspect=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:03.649726Z",
     "start_time": "2022-08-25T09:41:03.247119Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], s=5, alpha=0.8,\n",
    "            c=dih[idx_samples], cmap='jet')\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "cbar = plt.colorbar(pad=0.02)\n",
    "cbar.set_label(r\"Dihedral angle ($^\\circ$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kpca_intro'></a>\n",
    "## Nonlinear dimensionality reduction: kernelizing PCA üè°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many real-world problems, a linear approximation is not enough to learn the patterns in the data and obtain a meaningful compact representation. Indeed, this is one of the PCA limitations. One possible way to circumvent this problem is to apply a nonlinear transformation that maps the original data into a higher-dimensional feature space whereby the data can be linearly separable or easily split into clusters (see an example in the video below). At a first glance, this idea may sound as a contradiction, since we are interested in reducing the dimensionality of the data rather than increasing it. However, if the transformation is applied properly, it turns out that the data should lie on a lower-dimensional manifold of the high dimensional data space generated by the transformation. In other words, we increase the dimensionality in order to be able to reduce it to obtain a meaningful representation of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T18:31:13.558304Z",
     "start_time": "2022-07-26T18:31:13.438048Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "vid = YouTubeVideo('3liCbRZPrZA', width=600, height=450)\n",
    "display(vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, this transformation is performed via a *kernel function* which represents a nonlinear mapping of a sample $\\mathbf{X}$ into a new feature space calculated as a dot product in the form $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i)^T \\Phi(\\mathbf{x}_j)$, where $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are different data points. This approach gives rise to the so-called **[kernel principal component analysis](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis)** (or KPCA for short) that can be considered as an extension of the \"classic\" PCA method. Due to the scalar product, the kernel function can be also viewed as a *similarity measure* between pair of data points. In KPCA, the covariance matrix can be expressed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Cov = \\frac{1}{m} \\sum_{i=1}^m \\Phi(\\mathbf{x}_i)^\\top \\Phi(\\mathbf{x}_i).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, instead of solving the eigenvalue problem directly for the covariance matrix as in PCA, it can be shown that in the kernel PCA algorithm the eigenvector equation is solved for the (centered) kernel matrix $\\mathbf{K}$. Since we need to evaluate all pairwise dot product between data points, the computational cost of building the KPCA's kernel matrix scales with the number of samples in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some popular kernel functions:**\n",
    "\n",
    "- polynomial $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j +c_0)^d$\n",
    "- gaussian or radial basis functions (RBF) $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp{ \\left(\\frac{-||\\mathbf{x}_i - \\mathbf{x}_j||_2^2}{2 \\sigma^2} \\right)} = \\exp{ \\left(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||_2^2 \\right)}$, where $||\\, . ||_2$ denotes the $l_2$ vector norm\n",
    "- sigmoid $\\rightarrow K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh( \\gamma \\mathbf{x}_i^\\top \\mathbf{x}_j + c_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more details about kernel PCA, check out the links provided below:**\n",
    "\n",
    "- [Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models](https://arxiv.org/abs/1207.3538)\n",
    "- [Lecture by David R. Thompson](https://www.youtube.com/watch?v=HbDHohXPLnU&ab_channel=caltech)\n",
    "- [Kernel PCA tutorial](https://www.youtube.com/watch?v=qC2GeVWSivw&ab_channel=TsungTaiYeh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the \"magic\" behind the kernel transformation, let's start with a simple example by considering 2D toy dataset where we can apply both PCA and kernel PCA methods to see in practice how they perform on nonlinear data. First, we will use the sklearn package to generate a synthetic 2D dataset that consists of concentric circles with some gaussian noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:32:59.624177Z",
     "start_time": "2022-08-09T14:32:59.619383Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# The y variable stores the labels that assign each point to the outer or inner circle \n",
    "X, y = make_circles(n_samples=500, random_state=51, noise=0.1, factor=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now store the noisy-circles data into a pandas dataframe object to facilitate the anaysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:33:02.127082Z",
     "start_time": "2022-08-09T14:33:02.118700Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_circles = pd.DataFrame(X, columns=['x1','x2'])\n",
    "df_circles['class'] = y\n",
    "df_circles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:33:06.074479Z",
     "start_time": "2022-08-09T14:33:05.903030Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.scatterplot(data=df_circles, x=\"x1\", y=\"x2\", hue=\"class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, one can clearly see that there is no linear function able to separate the red and blue points. Furthermore, each circle has an isotropic distribution, meaning that the two features defining the circle, x1 and x2, have approximately same variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:33:10.414522Z",
     "start_time": "2022-08-09T14:33:10.404604Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in df_circles['class'].unique():\n",
    "    var_x1 = df_circles[df_circles['class'] == c]['x1'].var()\n",
    "    var_x2 = df_circles[df_circles['class'] == c]['x2'].var()\n",
    "    print('Circle {}'.format(c))\n",
    "    print(\"=\" * 10 + \"\\n\")\n",
    "    print(\"var(x1) = {:2.3f}\".format(var_x1))\n",
    "    print(\"var(x2) = {:2.3f}\".format(var_x2))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of these features, PCA is expected to fail in finding any meaningful projection of the circle data, and even if no projection is applied, the PCA transformation is still not very helpful as we will see in a moment. Anyways, since we want to see the effect of incorporating nonlinear effects in PCA via kernelization, we will perform the PCA analysis for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:41:52.830166Z",
     "start_time": "2022-08-09T14:41:52.822500Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df_circles[['x1','x2']].values\n",
    "pca = PCA(n_components=2)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# Now we add the principal components to the original \n",
    "# dataset, so we can visualize both results together\n",
    "df_circles['PC1'] = X_transformed[:,0]\n",
    "df_circles['PC2'] = X_transformed[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply the KPCA method implemented in the scikit-learn package on our dataset. Among the options ok kernel available in this package, here we will use the radial basis function as an example. For more details about different kernel approximations used in scikit-learn see the link:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/kernel_approximation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:44:01.764517Z",
     "start_time": "2022-08-09T14:44:01.715804Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=2, kernel=\"laplacian\", gamma=1)\n",
    "X_transformed = kpca.fit_transform(X)\n",
    "\n",
    "df_circles['KPC1'] = X_transformed[:,0]\n",
    "df_circles['KPC2'] = X_transformed[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T14:45:43.739032Z",
     "start_time": "2022-08-09T14:45:43.425420Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))\n",
    "\n",
    "# Plot of the original data points\n",
    "ax1.set_title(\"Original space\")\n",
    "p1 = sns.scatterplot(data=df_circles, x=\"x1\", y=\"x2\", hue=\"class\", ax = ax1)\n",
    "p1.text(0.80, 0.85, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p1.text(0.25, 0.45, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "ax1.legend_ = None\n",
    "\n",
    "# Plot of the data points transformed by KPCA\n",
    "ax2.set_title(\"Projected by KPCA\")\n",
    "p2 = sns.scatterplot(data=df_circles, x=\"KPC1\", y=\"KPC2\", hue=\"class\", ax = ax2)\n",
    "p2.text(-0.25, 0.30, \"A\", horizontalalignment='left', \n",
    "        size='large', color='black', weight='semibold')\n",
    "p2.text(0.40, 0.40, \"B\", horizontalalignment='left',\n",
    "        size='large', color='black', weight='semibold')\n",
    "ax2.legend(bbox_to_anchor=(1.15, 1.0),borderaxespad=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tutor2_kpca'></a>\n",
    "## Tutorial II: nonlinear compression of MD data with kernel PCA üñ•Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T15:47:29.459469Z",
     "start_time": "2022-08-24T15:45:32.955749Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca_2d = KernelPCA(n_components=2, kernel='rbf')\n",
    "kpca_2d.fit(X_train)\n",
    "X_transformed = kpca_2d.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T15:47:29.844683Z",
     "start_time": "2022-08-24T15:47:29.462108Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epot_train = data['E'][idx_samples]\n",
    "epot_min = np.min(data['E'])\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1], s=4, alpha=0.8,\n",
    "            c=epot_train-epot_min, cmap='jet')\n",
    "plt.xlabel(\"KPC1\")\n",
    "plt.ylabel(\"KPC2\")\n",
    "cbar = plt.colorbar(pad=0.02)\n",
    "cbar.set_label(\"Relative energy (kcal/mol)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To practice üí™:**\n",
    "\n",
    "**Use the code cells above as example to test the kernel PCA projection with different values of the gaussian width (see gamma hyperparameter in sklearn documentation) used in the RBF kernel. You can test values in the range of 0.01 to 0.02. Try also different kernels (e.g., laplacian and sigmoid kernels). How sensible is the distribution of the projected data with respect to changes in these hyperparameters?**\n",
    "\n",
    "If you would like to know more about hyperparameter selection in kernel principal component analysis, check out this paper: http://thescipub.com/abstract/10.3844/jcssp.2014.1139.1150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clustering'></a>\n",
    "# Clustering analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Clustering**](https://en.wikipedia.org/wiki/Cluster_analysis) refers to a broad class of unsupervised  learning techiniques used to recognize subgroups, or clusters, of points in a data set that are related to each other and different from other groups of points based on some **similarity criteria**. In this case, the algorithm's goal is to programmatically identify groups of data points that are coherent internally, i.e. sharing a high similarity score, but clearly different from each other. The greater the similarity (or homogeneity) within a group and the greater the difference between groups, the better the result of the clustering algorithm. This notion of similarity is present in all clustering algorithms that differ from one another essentially in relation to the mathematical formulation of what constitutes a cluster and how to efficiently find them. Popular clustering approaches include:\n",
    "\n",
    "- **Centroid-based** $\\rightarrow$ organizes the data into non-hierarchical clusters (K-Means, K-Medians)\n",
    "- **Density-based** $\\rightarrow$ connects areas of high sample density into clusters (DBSCAN, OPTICS)\n",
    "- **Hierarchical** $\\rightarrow$ creates a tree of clusters (Agglomerative)\n",
    "- **Distribution-based** $\\rightarrow$ assumes data is composed of distributions, such as Gaussian distributions (Gaussian mixture models)\n",
    "\n",
    "Evaluating the performance of clustering models is a quite challenging task since in most of the cases there is no information about classes distribution in the data. Considering only its internal structure, a \"good\" clustering usually involves a certain trade-off between *compactness* (intra-cluster cohesion) and *separability*. In addition, one might also evaluate a clustering algorithm based on the utility of the clustering in its intended application.\n",
    "\n",
    "Unlike the dimensionality reduction techiniques discussed before, clustering analysis can be applied directly in the original *n*-dimensional feature space of the data to discover its structure, in this case, distinct clusters.\n",
    "\n",
    "For an extensive list of clustering models, see [A Comprehensive Survey of Clustering Algorithms](https://link.springer.com/article/10.1007/s40745-015-0040-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kmeans'></a>\n",
    "## K-Means: a brief introduction üè°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is one of the widely used algorithms for clustering analysis. This method performs a hard partition of the data into a predefined number of distinct (and non-overlapping) clusters, $\\mathbf{k}$, by assigning each point $\\mathbf{x}_i$ to the cluster with the nearest centroid. Here the centroid's position is given by the mean of all data points assigned to a given cluster. The algorithm start by selecting *k* centroids $C_1,...,C_k$ corresponding to the number of clusters desired. Then each data point is assigned to the closest centroid by computing the Euclidean distance with respect to all centroids to form the initial clusters. In the next, the centroids' position are updated according to the mean value of all data points assigned to each centroid in the previous step. These two steps are repeated iteratively until the centroids remain the\n",
    "same, or match the previous iteration‚Äôs assignment. Thus, the idea behind K-means is to minimize the *within-cluster variation* given by the [sum of the squared error (SSE)](https://en.wikipedia.org/wiki/Residual_sum_of_squares) between each data point and its closest centroid, which is a measure of how much the samples within a cluster differ from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To know more about k-means, watch these lectures:**\n",
    "\n",
    "- [Lecture 13.2 of the Machine Learning course (coursera) by Andrew Ng](https://www.youtube.com/watch?v=hDmNF9JG3lo&ab_channel=ArtificialIntelligence-AllinOne)\n",
    "- [StatQuest: K-Means clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means algorithm is also part of the giant scikit-learn library, which provides a very efficient code implementation. So here, instead of rewritting the K-Means algorithm from scratch, we will go through the first iteration of the optimization process step-by-step such that we can gain a better understanding of how the algorithm works in practice. Let's do it!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unfolding the K-Means algorithm step-by-step**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to build a simple 2D data set having two randomly generated clusters in order to be able to visualize the steps of the algorithm using scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:17:54.176536Z",
     "start_time": "2022-08-19T06:17:54.166658Z"
    }
   },
   "outputs": [],
   "source": [
    "n_points = 25\n",
    "center1 = [2,3]\n",
    "center2 = [9,10]\n",
    "X = np.repeat([center1, center2], [n_points, n_points], axis=0)\n",
    "# Create two distinct blobs of points\n",
    "X = X + np.random.randn(*X.shape)\n",
    "# Stack the two cluster centers into one matrix\n",
    "orig_centroids = np.array([center1, center2])\n",
    "# Define the ground-truth labels for cluster evaluation\n",
    "labels = np.array(['blue'] * n_points + ['red'] * n_points)\n",
    "# Use a pandas DataFrame object as a container\n",
    "df_clusters = pd.DataFrame(X, columns=['x','y'])\n",
    "# Create a column of labels to be updated by K-Means\n",
    "df_clusters['labels'] = None\n",
    "df_clusters.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 1:** Assign initial centroids \"randomly\" for the two clusters in the data set. Note that in this simple example we already know how many clusters we have (K=2), but this information is generally not known a priori. We will back to this point later. \n",
    "\n",
    "In order to guarantee that the initialized centroids will be not placed too far apart from the data points, we will select randomly the x and y coordinates of each centroid constrained by the limits (max and min) of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:18:01.656381Z",
     "start_time": "2022-08-19T06:18:01.477357Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_max, y_max = df_clusters['x'].max(), df_clusters['y'].max()\n",
    "x_min, y_min = df_clusters['x'].min(), df_clusters['y'].min()\n",
    "\n",
    "init_center1 = np.array([np.random.uniform(x_min,x_max), np.random.uniform(y_min,y_max)])\n",
    "init_center2 = np.array([np.random.uniform(x_max,x_min), np.random.uniform(y_max,y_min)])\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 1\", fontsize = 18)\n",
    "plt.scatter(df_clusters.x, df_clusters.y, c=\"k\")\n",
    "plt.scatter(init_center1[0],init_center1[1], c=\"k\", edgecolor='yellow', s=150, marker=\"*\")\n",
    "plt.scatter(init_center2[0],init_center2[1], c=\"k\", edgecolor='yellow', s=150, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 2:** Now we need to calculate the Euclidean distance between *each data point in X and each point in the matrix of initialized centroids*. Note that this step is the most computationally expensive part of the K-Means algorithm because it involves (in the worst scenario) two loops with N iterations, where N is the number of samples in the data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid doing an expensive loop over all data points, we can do some reshaping in the centroids matrix to enable broadcasting with Numpy, in order to perform all the Euclidean distances calculations much faster, in a single shoot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:18:06.854724Z",
     "start_time": "2022-08-19T06:18:06.849780Z"
    }
   },
   "outputs": [],
   "source": [
    "centroids = np.array([init_center1, init_center2])\n",
    "centroids[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:18:09.141144Z",
     "start_time": "2022-08-19T06:18:09.135286Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_distances(data, centroids):\n",
    "    return np.linalg.norm(data - centroids[:, None], axis=2)\n",
    "\n",
    "d = compute_distances(X, centroids)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-18T16:03:57.897099Z",
     "start_time": "2022-08-18T16:03:57.892325Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>üìå Info: </b> Note that the distance matrix above has a shape of (2, n_points * 2), where each row corresponds to a given cluster centroid and stores the distances with respect to all the data points. In other words, the matrix d represents two stacked arrays that are each of same length as the input X.     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 3:** Next, we want to assign a label to every data point based on the closest cluster centroid. To do that, we need to find the minimum distance on the 0th axis from the distance matrix *d* calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:18:12.397221Z",
     "start_time": "2022-08-19T06:18:12.388766Z"
    }
   },
   "outputs": [],
   "source": [
    "current_labels = np.argmin(d, axis=0)\n",
    "df_clusters['labels'] = current_labels\n",
    "labels_mapping = {0: 'blue', 1: 'red'}\n",
    "df_clusters['labels'] = df_clusters['labels'].map(labels_mapping)\n",
    "df_clusters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:21:59.068664Z",
     "start_time": "2022-08-19T06:21:58.889105Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 3\", fontsize = 18)\n",
    "sns.scatterplot(data=df_clusters, x=\"x\", y=\"y\", hue=\"labels\")\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c=\"k\", edgecolor='yellow', s=150, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T13:06:22.232154Z",
     "start_time": "2020-12-23T13:06:22.228222Z"
    }
   },
   "source": [
    "**STEP 4:** Update the position of the centroids by computing the mean value of the newly formed clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T06:20:57.745025Z",
     "start_time": "2022-08-19T06:20:57.554691Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "centroids_new = df_clusters.groupby([\"labels\"]).mean()[[\"x\",\"y\"]].values\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.title(\"K-Means - STEP 4\", fontsize = 18)\n",
    "sns.scatterplot(data=df_clusters, x=\"x\", y=\"y\", hue=\"labels\")\n",
    "plt.scatter(centroids_new[:,0], centroids_new[:,1], c=\"k\", edgecolor='yellow', s=150, marker=\"*\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP 5:** Repeat step 2 to 4 until the positions of the centroids do not change within a given threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the K-Means algorithm is quite simple and easy to understand. The animation below illustrates how the algorithm iteratively converges to find some cluster structure in the data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans_gif](./figs/kmeans_anim.gif \"k-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tutor3_kmeans'></a>\n",
    "## Tutorial III: clustering MD data with K-Means üñ•Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to apply the K-Means algorithm to our molecular dynamics data set by using the efficient implementation available in scikit-learn. To do so, the first step is to decide **how many clusters we will search for**. Remember that K-Means is a hard partitioning method that requires prior knowledge of the number of clusters existing in the data. As we will see later, there are some heuristics to try to find the optimal number of cluster for K-Means (which also work for other methods), but for now let's use the intution we have already gained with the dimensionality reduction analysis. According to PCA and KPCA, it seems that the aspirin data set can be divided in two big and well-separated cluster with a smaller cluster between them. Therefore, we will try **k = 3 as our first guess** for K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:39.546000Z",
     "start_time": "2022-08-25T09:41:39.543320Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Instantiate the KMeans class with the following arguments:\n",
    "kmeans = KMeans(n_clusters=3, n_init=50, max_iter=100, random_state=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:39.793325Z",
     "start_time": "2022-08-25T09:41:39.790049Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the parameter *n_cluster* that defines the number of clusters to be searched, the two most important parameters of KMeans class are:\n",
    "\n",
    "- *n_init* $\\longrightarrow$ sets the number of initializations to perform. This is important because two runs can converge on different cluster assignments. The default behavior for the scikit-learn algorithm is to perform ten k-means runs and return the results of the one with the lowest value of the cost function (sum of squared estimate of errors, SSE).\n",
    "\n",
    "- *max_iter* $\\longrightarrow$ sets the number of maximum iterations for each initialization of the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the model, we can apply the *fit* method to find the clusters in our molecular dataset. Here we will use the same preprocessed data as in Tutorial I (i.e, molecular geometries represented by the inverse R2 descriptor followed by a normalization with standard scaler). If you have lost the **X_train variable** in the notebook, please go back to [section 3.2](#tutor1_pca) and execute the code cells again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:49.445626Z",
     "start_time": "2022-08-25T09:41:40.606655Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans.fit(X_train)\n",
    "kmeans.labels_ = kmeans.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics from the initialization run with the lowest SSE are available as attributes of K-Means after calling the .fit() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:49.456519Z",
     "start_time": "2022-08-25T09:41:49.450122Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Lowest SSE value: {}\".format(kmeans.inertia_))\n",
    "print(\" \")\n",
    "print(\"NumPy array with the converged centroid positions:\\n\")\n",
    "print(kmeans.cluster_centers_.shape)\n",
    "print(\" \")\n",
    "print(\"Number of iterations until convergence: {}\".format(kmeans.n_iter_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can obtain the cluster assignments (which correspond to the K-Means model predictions) as a one-dimensional NumPy array by using the *kmeans.labels_* attribute as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:49.467459Z",
     "start_time": "2022-08-25T09:41:49.458884Z"
    }
   },
   "outputs": [],
   "source": [
    "for n, k in enumerate(np.unique(kmeans.labels_)):\n",
    "    count = (kmeans.labels_ == k).sum()\n",
    "    print(f\"Number of geometries in cluster {n} = {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical analysis of the clustering results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the cluster labels information at hand, we can now analyze how much the groups of geometries identified by the K-Means algorithm differ from one another with respect to some property available in the molecular dataset. Here it is worth noting that one can use any chemical property, not necessarily the geometric features implicitly used as input for the model such as bond distances, angles, or dihedrals. To facilitate the analysis, let us first group some relevant chemical information available for the selected 10k geometries into a single Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:49.483889Z",
     "start_time": "2022-08-25T09:41:49.469968Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure that idx_samples correspond to the same one \n",
    "# used to build the X_train dataset for K-Means\n",
    "epot_train = data['E'][idx_samples]\n",
    "epot_min = np.min(data['E'])\n",
    "erel = (epot_train-epot_min).flatten()\n",
    "dih_samples = dih[idx_samples]\n",
    "\n",
    "df_kmeans = pd.DataFrame({'erel': erel, 'dih_cooh': dih_samples, \n",
    "                          'kmeans_pred': kmeans.labels_})\n",
    "df_kmeans.index = idx_samples\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By grouping the data according to the cluster's labels, one can calculate the basic statistical descriptors for all the properties stored in the dataframe above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:49.510155Z",
     "start_time": "2022-08-25T09:41:49.485722Z"
    }
   },
   "outputs": [],
   "source": [
    "df_kmeans.groupby('kmeans_pred').describe().reset_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>üìù Important:</b> In general, clustering methods are often applied to a dataset in its original (possibly, high-dimensional) representation without the need for any data projection to a lower dimensional space prior to the clustering analysis. Since, in most real case studies, the number of features in the data (dimensions) is much higher than three, one can not simply analyze the clustering results with a scatter plot as we did in section 3.2 in the PCA analysis. In this case, the most straightforward way to check the quality of the clustering analysis is by plotting histograms (or boxplots) of the desired quantity for each cluster, which may reveal relevant statistical differences among the groups.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:41:50.267887Z",
     "start_time": "2022-08-25T09:41:49.511406Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "h1 = sns.histplot(data=df_kmeans, x=\"erel\", hue='kmeans_pred', kde=True, \n",
    "                  palette=\"Set2\", ax=axes[0])\n",
    "h1.set(title = \"Relative energy\")\n",
    "h2 = sns.histplot(data=df_kmeans, x=\"dih_cooh\", hue='kmeans_pred', kde=True, \n",
    "                  palette=\"Set2\", ax=axes[1])\n",
    "h2.set(title = \"Dihedral angle of COOH group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining K-Means results with PCA for visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already know how to project the data into a 2D plane using PCA, we can combine this reduced representation with the K-Means results to color the scatter plot according to the cluster labels. In this way, we can get an intuition on how the three groups identified by K-Means are distributed in space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:42:11.971435Z",
     "start_time": "2022-08-25T09:42:11.701959Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_n2 = PCA(n_components=2)\n",
    "pca_n2.fit(X_train)\n",
    "X_transformed = pca_n2.transform(X_train)\n",
    "\n",
    "df_pca = pd.DataFrame(X_transformed, columns = ['PC1', 'PC2'])\n",
    "df_pca['kmeans_pred'] = kmeans.labels_\n",
    "df_pca.index = idx_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-25T09:42:12.599645Z",
     "start_time": "2022-08-25T09:42:12.177928Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,7))\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='kmeans_pred', \n",
    "                palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='optimal_k'></a>\n",
    "## Evaluation metrics for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-15T17:43:38.281270Z",
     "start_time": "2021-01-15T17:43:38.265197Z"
    }
   },
   "source": [
    "A common problem in clustering analysis is *how to evaluate the appropriateness of the various data partitions* provided by a clustering algorithm. This is an issue because, in contrast to supervised learning, there is no ground truth available in most cases to judge the validity of clustering results. Moreover, the data is often high dimensional, so the clustering patterns can not be identified by simple visual inspection of the plotted data. Finally, some of the clustering algorithms (including K-Means) require prior knowledge of the number of cluster in which the data will be partitioned.\n",
    "\n",
    "To mitigate these problems, several metrics have been proposed to compare different clustering algorithms and also to seek the optimal number of clusters in a given dataset. Typically, the clustering evaluation metrics can be divided into *internal*, when only the input data and the cluster labels are used, and *external*, where ground truth labels are available and can be used *a posteriori* for the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ For a more complete list of clustering evaluation metrics with practical examples, check out the scikit-learn documentation at https://scikit-learn.org/stable/modules/clustering.html#rand-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Internal evaluation metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img style=\"float: center;\" src=\"./figs/cohesion-x-separation.png\" width=\"550\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics usually try to encode the notion of compactness (or cluster cohesion) and inter-cluster spread (or cluster separation) into a single number based on the distribution of the inputs within the clusters. Some of the most used internal metrics for assessing the \"goodness\" of the resulting clusters are\n",
    "\n",
    "- [**Elbow method**](https://www.youtube.com/watch?v=lbR5br5yvrY)\n",
    "- [Silhouette coefficient](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "- [**Davies Bouldin indicator**](https://ieeexplore.ieee.org/document/4766909)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will see first an example of how to use the **Elbow method** to estimate the \"correct\" number of clusters in K-Means. The idea behind this method consists of running K-Means multiple times with an incremental number of clusters, $k$. Then, plotting the value of the loss function (i.e., the SSE value) as a function of $k$, we try to find the point at which the SSE curve starts to flatten out forming an \"elbow\". The Python code to run the Elbow method is quite simple as shown in the block cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:31:20.351710Z",
     "start_time": "2022-08-24T17:31:08.368211Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans_params = {\"init\": \"random\", \"n_init\": 20, \n",
    "                 \"max_iter\": 100, \"random_state\": 13,}\n",
    "\n",
    "# A list holds the SSE values for each k\n",
    "sse = []\n",
    "k_list = range(1, 10)\n",
    "for k in k_list:\n",
    "    model = KMeans(n_clusters=k, **kmeans_params)\n",
    "    model.fit(X_train)\n",
    "    sse.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:31:20.496689Z",
     "start_time": "2022-08-24T17:31:20.353450Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(k_list, sse, \"r-o\")\n",
    "plt.xticks(k_list)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot shown above, one can see that the SSE values always decrease when increasing k. As more centroids are added, the distance from each point to its closest centroid will decrease. Thus, the idea is to find a point in the plot that results in a reasonable trade-off between error and the number of clusters. In this example, the Elbow method gives k=2 (or maybe k=3) as the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always helpful (and recommended) to use at least two different metrics two evaluate the appropriateness of the clustering results to be able to choose the optimal number of clusters in the data with higher confidence. Here we will use another internal metric called **Davies-Bouldin indicator**, which is defined as the average similarity of each cluster with a cluster most similar to it, where similarity is the ratio of within-cluster distances to between-cluster distances. In this metric, the lower the value the better the clustering, with zero being the minimum score. Let's see below how to use sklearn to compute the DB score and use this metric to search for the optimal number of clusters for K-Means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:31:33.465596Z",
     "start_time": "2022-08-24T17:31:20.498532Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "scores = []\n",
    "# Since the DB score is based on the comparison between clusters,\n",
    "# we need to consider at minimum two clusters for K-Means. \n",
    "k_list = range(2, 10)\n",
    "for k in k_list:\n",
    "    model = KMeans(n_clusters=k, **kmeans_params)\n",
    "    model.fit(X_train)\n",
    "    dbs = davies_bouldin_score(X_train, model.labels_)\n",
    "    scores.append(dbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:31:33.630675Z",
     "start_time": "2022-08-24T17:31:33.470628Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(k_list, scores, \"r-o\")\n",
    "plt.xticks(k_list)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"DB score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above confirms that K=2 is in fact the ideal number of clusters to partition the aspirin MD dataset. However, this does not necessarily mean that K-Means with K=2 identifies the \"correct\" distribution of molecular configurations in the data. Indeed, we have seen that there is a group of points corresponding to transition state geometries (COOH dihedral angle varying in the range of 75$^\\circ$ to 105$^\\circ$) which K-Means is not able to properly group in the same cluster. This problem may be related to the limitations of K-Means since the algorithm is not effective in identifying clusters with different sizes, densities and shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**External evaluation metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The external metrics are generally classification-oriented mesures whose the task is to compare the cluster assignments of each point provided by the algorithm with a list of ground truth labels available for the data. These reference labels should be a discrete variable characterizing a certain number of classes associated with the data. Then, we can compute an *external criterion* that evaluates how well the clustering matches the standard reference classes. Some of the available evaluation metrics based on an external criterion are:\n",
    "\n",
    "- [**Rand index**](https://en.wikipedia.org/wiki/Rand_index)\n",
    "- [Fowlkes‚ÄìMallows indicator](https://en.wikipedia.org/wiki/Fowlkes%E2%80%93Mallows_index#:~:text=The%20Fowlkes%E2%80%93Mallows%20index%20is,metric%20to%20measure%20confusion%20matrices.)\n",
    "- [Normalized mutual information](http://proceedings.mlr.press/v32/romano14.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here we will calculate the **Rand index** by comparing the K-Means labels with a set of true labels that correspond to a discretization of the COOH dihedral variable into three ranges of angles: 0$^\\circ$ to 70$^\\circ$, 70$^\\circ$ to 110$^\\circ$, and 110$^\\circ$ to 180$^\\circ$. To create the reference class labels for the dihedrals, we will use the *.cut()* method of Pandas as demonstrated in the code cell below. \n",
    "\n",
    "> *The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.*<br><br> RI = (number of agreeing pairs) / (number of pairs) <br><br>\n",
    "[from sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html?highlight=rand+index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:32:15.536617Z",
     "start_time": "2022-08-24T17:32:15.527259Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_kmeans['dih_classes'] = pd.cut(df_kmeans['dih_cooh'],bins=[0,70,110,180],labels=[0,1,2]).values\n",
    "df_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-24T17:38:04.893201Z",
     "start_time": "2022-08-24T17:38:04.886579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import rand_score\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "\n",
    "ri = rand_score(df_kmeans['dih_classes'].values, df_kmeans['kmeans_pred'].values)\n",
    "ari = adjusted_rand_score(df_kmeans['dih_classes'].values, df_kmeans['kmeans_pred'].values)\n",
    "print(ri, ari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1661426830587,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
